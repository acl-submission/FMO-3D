import json
import os
import shutil
import numpy as np
import pandas as pd
from unimol_tools import MolTrain, MolPredict


def analyze_label_distribution(data):
    """è¿‡æ»¤æŽ‰å•ç±»åˆ«æ ‡ç­¾åˆ—ï¼Œä½†ä¿ç•™UNKæ ‡ç­¾"""
    targets = np.array(data['target'])
    
    # è¯»å–æ ‡ç­¾è¯æ±‡è¡¨
    with open("./datasets/openpom/aroma_vocabularies.json", "r") as f:
        vocab = json.load(f)

    # èŽ·å–å½“å‰çº§åˆ«
    level = "primary"
    if "secondary" in str(data.get('_level', '')):
        level = "secondary"
    elif "third" in str(data.get('_level', '')):
        level = "smells"

    labels = vocab[f"{level}_aroma"]['vocabulary']

    # æ‰¾åˆ° UNK çš„ä½ç½®
    try:
        unk_idx = labels.index("UNK")
    except ValueError:
        unk_idx = -1

    valid_cols = []
    for i in range(targets.shape[1]):
        unique_vals = np.unique(targets[:, i])
        # ä¿ç•™å¤šç±»åˆ«åˆ—æˆ–è€… UNK åˆ—
        if len(unique_vals) > 1 or i == unk_idx:
            valid_cols.append(i)

    # å¦‚æžœ UNK åˆ—å…¨æ˜¯0ï¼Œåˆ™äººå·¥åŠ ä¸€ä¸ªæ­£æ ·æœ¬
    if unk_idx in valid_cols:
        unk_vals = np.array([row[unk_idx] for row in data['target']])
        if np.all(unk_vals == 0):
            print("âš ï¸ UNKåˆ—å…¨æ˜¯0ï¼Œå¢žåŠ ä¸€ä¸ªæ­£æ ·æœ¬")
            # å¤åˆ¶ç¬¬ä¸€æ¡æ ·æœ¬
            new_sample = {k: (v[0] if isinstance(v, list) else v) for k, v in data.items() if k != '_level'}
            new_sample['target'] = data['target'][0].copy()
            new_sample['target'][unk_idx] = 1
            # åœ¨ data å„å­—æ®µæ·»åŠ 
            for k in data:
                if isinstance(data[k], list):
                    data[k].append(new_sample[k])

    data['_valid_columns'] = valid_cols
    data['target'] = [[row[i] for i in valid_cols] for row in data['target']]
    return data


def process_results(preds, smiles, level, output_dir, valid_columns=None, y_true=None):
    """ä¿å­˜é¢„æµ‹ç»“æžœä¸º CSVï¼ŒåŒ…å«çœŸå®žæ ‡ç­¾"""
    with open("./datasets/openpom/aroma_vocabularies.json", "r") as f:
        vocab = json.load(f)
    if level == "third":
        level = "smells"
    labels = vocab[f"{level}_aroma"]['vocabulary']

    if valid_columns and len(valid_columns) == len(preds[0]):
        labels = [labels[i] for i in valid_columns]

    print(f"ðŸ” {level} çº§åˆ«é¢„æµ‹ç»“æžœç»´åº¦: {len(preds[0])}, æ ‡ç­¾æ•°: {len(labels)}")

    rows = []
    for i, (s, p) in enumerate(zip(smiles, preds)):
        row = {"SMILES": s}
        # é¢„æµ‹æ¦‚çŽ‡å’Œé¢„æµ‹æ ‡ç­¾
        row.update({f"{lab}_probability": prob for lab, prob in zip(labels, p)})
        row.update({f"{lab}_prediction": int(prob > 0.5) for lab, prob in zip(labels, p)})
        # çœŸå®žæ ‡ç­¾ï¼ˆå¦‚æžœæä¾›äº† y_trueï¼‰
        if y_true is not None:
            row.update({f"{lab}_true": int(val) for lab, val in zip(labels, y_true[i])})
        rows.append(row)

    df = pd.DataFrame(rows)
    csv_path = os.path.join(output_dir, f"predictions_{level}.csv")
    df.to_csv(csv_path, index=False)
    print(f"âœ… CSV ä¿å­˜åˆ° {csv_path}, å½¢çŠ¶ {df.shape}")
    return csv_path



def train_and_predict(level="secondary"):
    train_file = f"./datasets/openpom/unimol_train_{level}.json" 
    test_file = f"./datasets/openpom/unimol_test_{level}.json"
    with open(train_file) as f: raw_train = json.load(f)
    with open(test_file) as f: test_data = json.load(f)

    # æ·»åŠ çº§åˆ«ä¿¡æ¯åˆ°æ•°æ®ä¸­ï¼Œä¾›analyze_label_distributionä½¿ç”¨
    raw_train['_level'] = level
    test_data['_level'] = level

    train_data = analyze_label_distribution(raw_train)
    valid_cols = train_data["_valid_columns"]

    print(f"ðŸ“‹ {level} çº§åˆ«: åŽŸå§‹æ ‡ç­¾æ•° {len(raw_train['target'][0])}, "
          f"æœ‰æ•ˆåˆ—æ•° {len(valid_cols)}")

    test_data['target'] = [[row[i] for i in valid_cols] for row in test_data['target']]
    
    # ðŸ”§ ä¿®å¤ï¼šåœ¨è°ƒç”¨ predict ä¹‹å‰ä¿å­˜ target æ•°æ®
    test_targets = test_data['target'].copy()

    model_dir, out_dir = f"exp_{level}", f"outputs_{level}"
    os.makedirs(model_dir, exist_ok=True); os.makedirs(out_dir, exist_ok=True)

    # ðŸ”§ ä¿®å¤ï¼šå¢žåŠ è®­ç»ƒè½®æ•°ï¼Œç¡®ä¿æ¨¡åž‹æ­£ç¡®ä¿å­˜
    clf = MolTrain(
        task="multilabel_classification", num_classes=len(valid_cols),
        batch_size=16, metrics="auc,acc", learning_rate=1e-3, epochs=200,  # å¢žåŠ åˆ°5ä¸ªepoch
        remove_hs=True, seed=42, model="unimolv2", model_size="large",
        save_path=model_dir, kfold=1, class_weight="balanced"
    )
    
    # ðŸ”§ ä¿®å¤ï¼šåªä¼ é€’UniMoléœ€è¦çš„å­—æ®µï¼Œè¿‡æ»¤æŽ‰é¢å¤–å­—æ®µ
    fit_data = {k: test_data[k] for k in ["SMILES","target","atoms","coordinates"]}
   
    clf.fit(data=fit_data)

    # ðŸ”§ ä¿®å¤ï¼šæ£€æŸ¥æ¨¡åž‹æ–‡ä»¶æ˜¯å¦æ­£ç¡®ä¿å­˜
    model_files = [f for f in os.listdir(model_dir) if f.endswith('.pth')]
    if not model_files:
        raise FileNotFoundError(f"âŒ æ¨¡åž‹æƒé‡æ–‡ä»¶æœªä¿å­˜åˆ° {model_dir}")
    
    print(f"âœ… æ¨¡åž‹å·²ä¿å­˜åˆ° {model_dir}, æ–‡ä»¶: {model_files}")

    # clf = MolPredict(load_model=model_dir)
    # preds = clf.predict(data=test_data, save_path=out_dir, metrics="auc,acc")

    # json_path = os.path.join(out_dir, f"predictions_{level}.json")
    # with open(json_path, "w") as f:
    #     json.dump({"SMILES": test_data["SMILES"], "predictions": preds.tolist()}, f, indent=2)
    # print(f"âœ… JSON ä¿å­˜åˆ° {json_path}")

    # # ðŸ‘‰ ä½¿ç”¨ä¿å­˜çš„ test_targets è€Œä¸æ˜¯ test_data["target"]
    # return process_results(preds, test_data["SMILES"], level, out_dir, valid_cols, y_true=test_targets)



if __name__ == "__main__":
    import sys
    levels = sys.argv[1:] if len(sys.argv) > 1 else ["secondary"]
    for lv in levels:
        train_and_predict(lv)
